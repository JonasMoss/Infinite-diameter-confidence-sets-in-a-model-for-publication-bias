\documentclass[twoside]{article}
\usepackage{arxiv}

%%% ============================================================================
%%% Macros.
%%% ============================================================================
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\argmin}{argmin}
\DeclareMathOperator{\Cor}{Cor}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\diag}{diag} 
\DeclareMathOperator{\argmax}{argmax}
\DeclareMathOperator{\supp}{supp}

\title{Infinite diameter confidence sets in Hedges' publication bias model}

\author{
  Jonas Moss \orcid{0000-0002-6876-6964} \\
  Department of Mathematics, University of Oslo\\
  PB 1053, Blindern, NO-0316, Oslo, Norway \\
  \it{jonasmgj@math.uio.no}
}

\titletag{An arXiv preprint, v2}
\makeatletter
\titlerunning{\@title}
\makeatother
\authorrunning{Jonas Moss}

\begin{document}
\maketitle
\begin{abstract}
Meta-analysis is the quantitative combination of results from different studies. The assumptions of classical meta-analysis models are not satisfied whenever publication bias is present, which causes inconsistent parameter estimates. A popular method to account for publication bias in meta-analysis is Hedges' selection function model, but frequentist estimation of this model is problematic. This paper shows there is no confidence set of guaranteed finite diameter for the mean and heterogeneity parameters in the selection function model. The proof is based on a generalization of the Gleser--Hwang theorem.
\end{abstract}

\keywords{meta-analysis \and publication bias \and selection function model \and p-hacking \and selection models \and random effects models \and weight function models}

\section{Introduction}
Meta-analysis is the quantitative combination of results from different studies with the goal of reducing sampling error \citep{lipsey2001practical}. 
Publication bias occurs when the published research literature is
not representative of the all the research that has actually been
done \citep{Rothstein2006-cq}. An important case of publication
bias is when publication decisions are made based on \textit{p}-values
\citep{sterling1959publication}, which, along its sister phenomenon \textit{p}-hacking \citep{simmons2011false},
can cause seriously biased conclusions when not accounted
for \citep{simmons2011false,moss2019modelling}.

The most important model for meta-analysis is the normal random effects
model with normal likelihoods \citep{hedges1998fixed}.
The likelihood for an observation in this model is $\phi(x_{i}\mid\theta_{0},(\sigma_{i}^{2}+\tau^{2})^{1/2})$,
where $\phi(x\mid\mu,\sigma)$ is the normal density with
mean $\mu$ and standard deviation $\sigma$. The parameter $\tau$ is the standard deviation of the random effects distribution, known as the \textit{heterogeneity parameter}. The $\sigma_i$s are the study-specific standard errors, assumed known, and $\theta_0$ is the mean of the effect size distribution. 


A modification of the random effects meta-analysis model due to \citet{hedges1984estimation}
allows us to account for \textit{p}-value based publication bias. Let $u=\Phi(-x_{i}/\sigma_{i})$ be the standard one-sided
\textit{p}-value for the null hypothesis $\theta_{i}=0$, and $0\leq w(u)\leq 1$ be
a probability for each $u$. The \textit{selection function meta-analysis
model} \citep{hedges1984estimation,hedges1992modeling} based on
the \emph{p}-value $u$ is
\begin{equation}
f(x_{i}\mid\theta_{0},(\sigma_{i}^{2}+\tau^{2})^{1/2})\propto\phi(x_{i}\mid\theta_{0},(\sigma_{i}^{2}+\tau^{2})^{1/2})w(u)\label{eq:Publication bias model}
\end{equation}
The model is also known as the \textit{weighting function publication bias
model}. The interpretation of this model is straightforward:
\begin{quote}
Alice is an editor who receives a study with \textit{p}-value $u$.
Her publication decision is a random function of this \textit{p}-value. That is, 
she will publish the study with some probability $w(u)$ and reject it with probability $1-w(u)$.
Every study you will ever read in Alice's journal has survived this
selection mechanism, the rest are lost forever.
\end{quote}

This is a rejection sampling procedure \citep{von1951various,flury1990acceptance} where $\phi$ serves as a proposal distribution for $f$. ((Too abrupt.)) The publication probability function is likely to be well approximated by a step function, with cutoffs such as $0.01,0.025$
and $0.05$ being especially important. ((cite something)). 

To model this, let $\alpha$
be a vector with $0=\alpha_{0}<\alpha_{1}<\cdots<\alpha_{J}=1$ and
$\rho$ be a non-negative vector in $[0,1]^{J}$ with $\rho_{1}=1$.
Now define 
\begin{equation}
w(u\mid\rho,\alpha)=\sum_{j=1}^{J}\rho_{j}1_{(\alpha_{j-1},\alpha_{j}]}(u).
\end{equation}
This is a step function where the value of $w$ on each step $(\alpha_{j-1},\alpha_{j}]$
is the probability of acceptance for a study with a \textit{p}-value
$u$ falling inside the interval $(\alpha_{j-1},\alpha_{j}]$. \citet{hedges1992modeling} discussed this variant of the selection function publication model, and we will refer to it as Hedges' publication bias model. %$\phi(x_{i}\mid\theta_{0},(\sigma_{i}^{2}+\tau^{2})^{1/2})w(u;\rho,\alpha)$ the step function publication bias model and denote it $f(x_{i}\mid\theta_{0},(\sigma_{i}^{2}+\tau^{2})^{1/2})$.

The selection function publication bias model is due to \citet{hedges1984estimation},
who used an $F$ distribution instead of a the normal distribution
and a single step in $w(u\mid\rho,\alpha)$. \citet{iyengar1988selection}
studied other choices of $w$ while \citet{citkowicz2017parsimonious}
used a beta density publication probability function instead of a
step function. 

Frequentist estimation of the step function publication bias model
is difficult, as noted by \citet[Appendix 1]{mcshane2016adjusting}.

(( Add something about ridges in the likelihood, maybe a plot.))
The purpose of this paper is to formalize and prove exactly how bad frequentist estimation of this model can be. This is done by proving there are no confidence sets of guaranteed finite diameter for the mean parameter $\theta_{0}$
and the heterogeneity parameter $\tau$ for any coverage $1-\alpha$.
This is a problematic result for two reasons. 
\begin{enumerate}[label=(\roman*)]
\item It would be hopeless to report confidence sets for $\tau^{2}$ like $[0.5,\infty)$, as they have no practical value. 
\item It shows that the automatic confidence
sets procedures that are guaranteed to yield finite confidence sets
of some positive nominal coverage, such as bootstrapped confidence
sets, likelihood-ratio based confidence sets, and subsampling confidence
sets never have true coverage greater than $0$ \citep[see][]{gleser996bootstrap}.
\end{enumerate}
The main result of this paper is the following theorem.
\begin{thm}
\label{thm:p-hacking infinite confidence interval}Let $h(x)=\prod_{i=1}^{n}f(x_{i}\mid\theta_{0},\tau,\sigma_{i},\rho)$ be the density of $n$ independent observations from a publication bias model. Then $h$ has no almost surely finite diameter confidence set for $\theta$ or $\sigma$ of non-zero coverage.
\end{thm}

The main ingredient in the proof of \cref{thm:p-hacking infinite confidence interval}
is \cref{thm:Infinite diameter main theorem}, an extension
and reformulation of the Gleser--Hwang theorem \citep{gleser1987nonexistence}. 
They used their theorem to prove the non-existence of confidence sets with
guaranteed finite diameter for models such as the errors-in-variables
regression model ((cite)) and Fieller's ratio of means problem ((cite)). Theorem 1 of \citet{berger1999integrated}
is a similar extension of the Gleser--Hwang theorem.

The problem with non-existence
of confidence sets with guaranteed finite diameter for finite-dimensional
parameters has not received much attention in the statistical literature,
as lamented by \citet{gleser996bootstrap}. Some references in this
area are \citet{bahadur1956nonexistence}, who studied infinitely
large confidence sets in non-parametric estimation of the mean, \citeauthor{romano2004non}'s
(2004) extension of their results, \citet{Donoho1988-hg} and \citet{Pfanzagl1998-fe}.

\section{Definitions and Proofs}
Let $P_{\theta},\theta\in\Theta$ be a parameterized family of dominated probability
measures with densities $p_{\theta}$. Let $g:\Theta\to E$ for some
set $E$ equipped with a positive function $\left\Vert \cdot\right\Vert :E\to[0,\infty)$.
Here $g$ maps $\theta$ to the parameter we wish to form a confidence
set for, sometimes called a \textit{focus parameter} ((cite Nils)). For instance, $g(\theta) = \theta$ or $g(\theta) = E_\theta(X)$, the expectation of some random variable $X$ calculated under $P_\theta$. 

The function $\left\Vert \cdot\right\Vert$ measures the size of each $e \in E$. It could be a norm if $E$ is a vector space. In \cref{thm:Infinite diameter main theorem} it is the absolute value $\left|\cdot\right|$. 

Recall the definition of confidence sets. A set $C$ is a confidence set for $g(\theta)$ with coverage probability $1-\alpha$
if $P_{\theta}(g(\theta)\in C)\geq1-\alpha$
for all $\theta$. 

Let ${A[g(\theta)]}$ be a family of level $\alpha$ acceptance sets. An acceptance set for $C$ with level $\alpha$ has the property than $P_{\theta}(A[g(\theta)])\geq1-\alpha$.
There is a well known duality between confidence sets and acceptance
sets. For each confidence set $C$ there is a collection of acceptance
sets $A[g(\theta)]$ satisfying $\omega\in A[g(\theta)]$ if and only if $g(\theta)\in C(\omega)$. The classical reference for these concepts is the book by \citet[Chapter 3.5]{lehmann2006testing}

We will need terminology for the diameter of the confidence set according
to the function $\left\Vert \cdot\right\Vert $. Define the diameter of $C$ as a random variable, $D=\textrm{sup}_{\eta\in C}\left\Vert \eta\right\Vert$.
A confidence set has infinite diameter at $\theta$ with positive
probability if $P_{\theta}(D=\infty)>0$. If $P_{\theta}(D=\infty)>0$
for all $\theta$, we will say that $D$ has infinite diameter with
positive probability.

\begin{lem}
Let $C$ be a level $\alpha$ confidence set and $A[g(\theta)]$ its associated
acceptance sets. If there exists a sequence $\left\{ \theta_{n}\right\} _{n=1}^{\infty}$
with $\left\Vert g(\theta_{n})\right\Vert \to\infty$ such
that $\lim_{n\to\infty}P_{\theta}(A[g(\theta_{n})])>0,$then
the diameter of $C$ at $\theta$ is infinite with positive probability.
\end{lem}
\begin{proof}
Assume without loss of generality that $\left\Vert g(\theta_{n})\right\Vert \geq n$
for each $n$; such a sequence can always be found by appropriately
filtering the original $\left\{ \theta_{n}\right\} _{n=1}^{\infty}$.
Since $\left\{ D\geq n\right\} $ is a decreasing sequence of sets,
$P_{\theta}(D=\infty)=\lim_{n\to\infty}P_{\theta}(D\geq n)$
for each $\theta$. From $A[g(\theta_{n})]\subseteq\left\{ D\geq n\right\} $
it follows that $P_{\theta}(A[g(\theta_{n})])\leq P_{\theta}(D\geq n)$,
thus $P_{\theta}(D=\infty)=\lim_{n\to\infty}P_{\theta}(D\geq n)\geq\lim_{n\to\infty}P_{\theta}(A[g(\theta_{n})])>0$.
\end{proof}
This is the extension of the Gleser--Hwang theorem. Here $\supp f = \{x\mid f(x) \neq 0\}$ denotes the support of $f$.
\begin{thm}
\label{thm:Infinite diameter main theorem}If there exists a sequence
$\left\{ \theta_{i}\right\} _{i\in\mathbb{N}}$ with $\left\Vert g(\theta_{i})\right\Vert \to\infty$
such that $p_{\theta_{i}}\to f^{\star}$ pointwise and $\supp p_{\theta_i}=\supp f^{\star}$
for each $\theta_i$, then every confidence set with coverage $1-\alpha>0$
has infinite diameter with positive probability.
\end{thm}
\begin{proof}
Assume without loss of generality that $\left\Vert g(\theta_{n})\right\Vert \geq n$.
By a variant of the dominated convergence theorem \citep[Exercise 16.4a]{billingsley1995probability}, $\int_{D\geq n}p_{\theta_{n}}d\mu\to\int_{D=\infty}f^{\star}d\mu$.
Since $A[g(\theta_{n})]\subseteq\left\{ D\leq n\right\}$, we have 
$1-\alpha\leq\int_{A[g(\theta_{n})]}p_{\theta_{n}}d\mu\leq\int_{D\geq n}p_{\theta_{n}}d\mu$
for all $n$, thus $\int_{D=\infty}f^{\star}d\mu>1-\alpha>0$. The
result follows from $P_{\theta}(D\geq n)=\int_{D\geq n}(p_{\theta}/p_{\theta_{n}})p_{\theta_{n}}d\mu>\int_{D=\infty}(p_{\theta}/f^{\star})f^{\star}d\mu>0$.
\end{proof}
This is an extension of \cref{thm:Infinite diameter main theorem}
to mixture distributions.
\begin{cor}
\label{cor:Mixture model corollary}Let $f_{\theta}^{1},f_{\theta}^{2},\ldots$
be a finite or infinite sequence of densities, $\pi(\theta)$
a countable probability vector and $\sum_{n=1}^{\infty}\pi_{n}(\theta)f_{\theta}^{n}$
a mixture distribution. Assume there is a sequence $\left\{ \theta_{i}\right\} _{i\in\mathbb{N}}$
with $\left\Vert g(\theta_{i})\right\Vert \to\infty$ such
that $f_{\theta_{i}}^{1}\to f^{\star}$ pointwise and $\supp f_{\theta}^{1} = \supp f^{\star}$
for each $\theta$ . If there is a parameter $\theta'\in\Theta$ satisfying
$\pi_{1}(\theta')>\alpha$, the mixture $\sum_{n=1}^{\infty}\pi_{n}(\theta)f_{\theta}^{n}$
admits no almost surely finite diameter confidence set of coverage
$1-\alpha$ for $g(\theta)$.
\end{cor}

\begin{proof}
Let $C$ be confidence set for $g(\theta)$ with coverage
$1-\alpha$. Since $\pi_{1}(\theta')>\alpha$ by assumption,
$C$ must include a confidence $C_{1}$ for $f_{\theta}^{1}$ of some
positive coverage. But by \cref{thm:Infinite diameter main theorem},
$C_{1}$ has infinite diameter with positive probability for all $\theta$.
Since $C\supseteq C_{1}$, $C$ has infinite diameter with positive
probability too.
\end{proof}
The following lemma is a proof of \cref{thm:p-hacking infinite confidence interval} for the special case when no non-significant studies are published.

% Let $\phi_{[a,b)}(x\mid\theta,\sigma)$ denote the density of a normal variable with mean $\theta$ and standard deviation $\sigma$ truncated to $[a,b)$.

\begin{lem} \label{lem:One-sided normal limit}
Let $f_n$ be a normal density truncated to $[c,\infty)$ with underlying mean $\theta_{n}=-n+c$ and standard deviation $\sigma_{n}=n^{1/2}$. Then $f_n(x)$ converges pointwise to $\exp(-x+c)$, the density of a shifted exponential.
\end{lem}
\begin{proof}
Using the well-known formula for the truncated normal we find that 
\begin{align*}
f_n(x) & = \phi(x; \theta_n, \sigma_n)\Phi\left(\frac{\theta_n - c}{\sigma_n}\right)^{-1}1[x>c], \\
 & = \phi(x;-n+c,n^{1/2})\Phi(-n^{1/2})^{-1}1[x>c].
\end{align*}
The normal density part equals 
\[
(2\pi)^{-1/2}n^{-\frac{1}{2}}\exp\left(\frac{-n^{2}-2n(x-c)+[2cx-c^{2}-x^{2}]}{2n}\right)
\]
When $n$ is large compared to $x$ and $c$, the term $(2cx-c^{2}-x^{2})/2n$
is negligible, hence 
\[
\phi(x;-n+c,n^{1/2})\approx(2\pi)^{-1/2}n^{-\frac{1}{2}}\exp(-n/2)\exp(-x+c)
\]
which equals $\exp(-x+c)\phi(n^{1/2})/n^{1/2}$.
Since $\Phi(-n^{1/2})\approx \phi(n^{1/2})/n^{1/2}$
as $n$ grows \citep[Equation 5]{borjesson1979simple},
we end up with $$\Phi(n^{1/2})^{-1}\phi(x;-n-c,n^{1/2})1_{x>c}\to\exp(-x+c),$$
the density of a shifted exponential.
\end{proof}

we will prove \cref{thm:p-hacking infinite confidence interval} by connecting \cref{cor:Mixture model corollary} about mixture distributions to \cref{lem:One-sided normal limit} above. To do this, we will need a mixture distribution representation of the publication bias model, which is given in the following lemma. We omit the proof as it is based purely on straight-forward calculations. 
\begin{lem}
\label{lem:Mixture representation} The density of an observation
from the publication bias model with parameter vector $(\theta_{0},\tau,\sigma_{i},\rho)$ can be written in a mixture form as
\begin{equation}
f(x;\theta_{0},\tau,\sigma_{i})=\sum_{j=1}^{N}\pi_{j}f_j(x;\theta_{0},(\tau^{2}+\sigma_{i}^{2})^{1/2}).\label{eq:Random effects publication bias}
\end{equation}
Here $f_j$ is a normal truncated to ${[\Phi^{-1}(1-\alpha_{j}),\Phi^{-1}(1-\alpha_{j-1}))}$. The mixture probabilities are
\[
\pi_{j}=\rho_{j}\frac{\Phi(c_{j-1}\mid\theta_{0},(\tau^{2}+\sigma_{i}^{2})^{1/2})-\Phi(c_{j}\mid\theta_{0},(\tau^{2}+\sigma_{i}^{2})^{1/2})}{\sum_{j=1}^{J}\rho_{j}[\Phi(c_{j-1}\mid\theta_{0},(\tau^{2}+\sigma_{i}^{2})^{1/2})-\Phi(c_{j}\mid\theta_{0},(\tau^{2}+\sigma_{i}^{2})^{1/2})]},
\]
where $c_i = \Phi^{-1}(1-\alpha_{i})$.
\end{lem}

\begin{proof}[Proof of \cref{thm:p-hacking infinite confidence interval}]
When $n=1$, the result follows by applying \cref{cor:Mixture model corollary} and \cref{lem:One-sided normal limit} to the mixture components $f_j(x;\theta_{0},(\tau^{2}+\sigma_{i}^{2})^{1/2})$ of \cref{eq:Random effects publication bias}.

For the case with more than one observation, observe that 
\[
\prod_{i=1}^{n}\sum_{j=1}^{N}\pi_{j}\phi_{[\Phi^{-1}(1-\alpha_{j}),\infty)}(x_{i}\mid\theta_{0},(\tau^{2}+\sigma_{i}^{2})^{1/2})
\]
is mixture model, which can be verified by expanding the expression. The component belonging to the mixture probability $\pi_{j}^{n}$ fulfills the demands of Proposition \ref{cor:Mixture model corollary} for $\alpha_{j}$ when $\alpha_{j+1}=1$, as its density is a product of densities converging to shifted exponentials.
\end{proof}


\section*{Acknowledgements}
I am grateful to Riccardo De Bin and Emil Aas Stoltenberg for reading through the manuscript and giving helpful comments.

%\printbibliography

\bibliography{infiniteci}
\end{document}
